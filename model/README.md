# üß† Servidor vLLM com Docker (CPU)

Este comando executa um **servidor vLLM** em um cont√™iner Docker para servir modelos de linguagem de forma r√°pida e eficiente, sem precisar instalar depend√™ncias locais de IA.  
O exemplo abaixo usa o modelo **Qwen2-1.5B-Instruct**, que √© leve e otimizado para rodar em CPU.

---

## üöÄ O que √© o vLLM?

O **vLLM** √© um **servidor de infer√™ncia otimizado para LLMs (Large Language Models)**.  
Ele fornece uma **API compat√≠vel com o padr√£o OpenAI** (`/v1/chat/completions`), permitindo usar bibliotecas como `requests`, `LangChain`, ou `OpenAI SDK` para se comunicar com o modelo ‚Äî assim como se fosse a API oficial da OpenAI, mas rodando **localmente**.

---

## üê≥ Comando para executar


```bash
cd model

bash vllm-cpu.sh
````


```bash
docker run -it \
  --rm \
  --network=host \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  vllm-cpu-env \
  --model Qwen/Qwen2-1.5B-Instruct \
  --trust-remote-code \
  --device cpu \
  --dtype bfloat16 \
  --tokenizer-mode auto
````

---

## ‚öôÔ∏è Explica√ß√£o dos par√¢metros

| Flag / Op√ß√£o                                       | Descri√ß√£o                                                                                                      |
| -------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| `docker run -it`                                   | Executa o cont√™iner em modo interativo.                                                                        |
| `--rm`                                             | Remove o cont√™iner automaticamente ao final da execu√ß√£o.                                                       |
| `--network=host`                                   | Usa a rede do host, permitindo acesso direto via `localhost`. Ideal para servir a API localmente.              |
| `-v ~/.cache/huggingface:/root/.cache/huggingface` | Monta o cache local do Hugging Face para evitar baixar o modelo novamente a cada execu√ß√£o.                     |
| `vllm-cpu-env`                                     | Nome da imagem Docker (deve conter o ambiente vLLM configurado).                                               |
| `--model Qwen/Qwen2-1.5B-Instruct`                 | Define o modelo a ser carregado.                                                                               |
| `--trust-remote-code`                              | Permite carregar c√≥digo customizado do reposit√≥rio do modelo (necess√°rio para alguns modelos da Hugging Face). |
| `--device cpu`                                     | Define que o modelo ser√° executado na CPU.                                                                     |
| `--dtype bfloat16`                                 | Tipo de dado usado na infer√™ncia (bfloat16 reduz o uso de mem√≥ria).                                            |
| `--tokenizer-mode auto`                            | Deixa o vLLM escolher o melhor modo de tokeniza√ß√£o automaticamente.                                            |

---

## üì° Endpoint gerado

Ap√≥s iniciar, o vLLM exp√µe um servidor HTTP compat√≠vel com OpenAI API:

```
http://localhost:8000/v1/chat/completions
```

Voc√™ pode testar com um `curl` ou com Python.

### Exemplo com `curl`:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2-1.5B-Instruct",
    "messages": [
      {"role": "system", "content": "Voc√™ √© um assistente √∫til."},
      {"role": "user", "content": "Explique o que √© o vLLM."}
    ]
  }'
```

### Exemplo com Python:

```python
import requests

url = "http://localhost:8000/v1/chat/completions"
payload = {
    "model": "Qwen/Qwen2-1.5B-Instruct",
    "messages": [
        {"role": "system", "content": "Voc√™ √© um assistente √∫til."},
        {"role": "user", "content": "Explique o que √© o vLLM."}
    ]
}
response = requests.post(url, json=payload)
print(response.json())
```

---

## üß© Requisitos

* Docker instalado e funcionando.
* A imagem `vllm-cpu-env` deve estar dispon√≠vel localmente ou no registro Docker.
* Conex√£o com a internet (apenas no primeiro download do modelo).

---

## üí° Dica

Se quiser usar uma GPU (quando dispon√≠vel), basta trocar:

```bash
--device cpu
```

por

```bash
--device cuda
```

E garantir que a imagem do Docker tenha suporte a CUDA.

---

## ‚úÖ Exemplo de integra√ß√£o com outro sistema

Voc√™ pode conectar esse servidor ao seu backend (como o FastAPI usado no projeto RAG):

```python
url = "http://localhost:8000/v1/chat/completions"
headers = {"Content-Type": "application/json"}
payload = {
    "model": "Qwen/Qwen2-1.5B-Instruct",
    "messages": [
        {"role": "system", "content": "Voc√™ √© um assistente √∫til."},
        {"role": "user", "content": "Resuma o conte√∫do do documento."}
    ]
}
response = requests.post(url, headers=headers, json=payload)
print(response.json())
```

---

## üß† Resumo

| Tarefa              | Descri√ß√£o                                   |
| ------------------- | ------------------------------------------- |
| üîß Subir o servidor | Executar o comando Docker acima             |
| üåê Endpoint local   | `http://localhost:8000/v1/chat/completions` |
| üí¨ Modelo usado     | `Qwen/Qwen2-1.5B-Instruct`                  |
| ‚öôÔ∏è Ambiente         | CPU (modo leve)                             |
| üì¶ Persist√™ncia     | Cache de modelos em `~/.cache/huggingface`  |

---

## üèÅ Conclus√£o

Esse comando permite executar um modelo de linguagem localmente com **vLLM** e **Docker**, servindo uma API OpenAI-compat√≠vel em poucos segundos ‚Äî sem precisar configurar ambientes complexos de machine learning.

```

---

Deseja que eu gere tamb√©m o **Dockerfile** correspondente √† imagem `vllm-cpu-env` (para voc√™ construir localmente)?  
Assim voc√™ poderia montar seu pr√≥prio ambiente vLLM customizado.
